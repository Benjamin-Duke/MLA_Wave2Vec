{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Wav2Vec2Config:\n",
    "    # Feature encoder\n",
    "    conv_layers: list = None\n",
    "    dropout: float = 0.1\n",
    "    layer_drop: float = 0.05\n",
    "    \n",
    "    # Transformer\n",
    "    d_model: int = 768\n",
    "    nhead: int = 8\n",
    "    num_encoder_layers: int = 12\n",
    "    dim_feedforward: int = 3072\n",
    "    \n",
    "    # Quantizer\n",
    "    num_groups: int = 2\n",
    "    num_vars: int = 320\n",
    "    temp: float = 2.0\n",
    "    min_temp: float = 0.5\n",
    "    temp_decay: float = 0.999995\n",
    "    \n",
    "    # Masking\n",
    "    mask_prob: float = 0.065\n",
    "    mask_length: int = 10\n",
    "    \n",
    "    # Training\n",
    "    learning_rate: float = 5e-4\n",
    "    warmup_steps_pct: float = 0.08  # 8% warmup\n",
    "    num_updates: int = 400_000  # BASE model updates\n",
    "    l2_weight: float = 0.1  # L2 penalty for encoder activations\n",
    "    encoder_grad_scale: float = 0.1  # Scale down encoder gradients\n",
    "    contrastive_temperature: float = 0.1  # κ in the paper\n",
    "    diversity_weight: float = 0.1  # α in the paper\n",
    "    num_negatives: int = 100  # K distractors\n",
    "    \n",
    "    @classmethod\n",
    "    def BASE(cls):\n",
    "        return cls(\n",
    "            conv_layers=[(512, 10, 5)] + [(512, 3, 2)] * 5 + [(512, 2, 2)],\n",
    "            layer_drop=0.05,\n",
    "            d_model=768,  # Ensures d/G = 384 for codebook entries\n",
    "            min_temp=0.5,\n",
    "            num_updates=400_000,\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def TINY(cls):\n",
    "        \"\"\"Tiny configuration for fast testing\"\"\"\n",
    "        return cls(\n",
    "            conv_layers=[(256, 10, 5)] + [(256, 3, 2)] * 3 + [(256, 2, 2)],  # Fewer layers, smaller channels\n",
    "            d_model=256,  # Smaller model dimension\n",
    "            nhead=4,      # Fewer attention heads\n",
    "            num_encoder_layers=4,  # Fewer transformer layers\n",
    "            dim_feedforward=1024,  # Smaller feedforward dimension\n",
    "            layer_drop=0.05,\n",
    "            num_groups=2,\n",
    "            num_vars=160,  # Smaller codebook\n",
    "            learning_rate=5e-4,  # Same as BASE but with faster schedule\n",
    "            num_updates=50_000,  # Much fewer updates for testing\n",
    "            warmup_steps_pct=0.1,  # Slightly faster warmup\n",
    "            mask_prob=0.1,  # Slightly higher masking probability\n",
    "            mask_length=5,  # Shorter mask length\n",
    "            min_temp=0.5,  # Same as BASE\n",
    "            temp_decay=0.9999,  # Faster temperature decay\n",
    "            l2_weight=0.01,  # Reduced L2 penalty\n",
    "            encoder_grad_scale=0.1,  # Same as BASE\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def LARGE(cls):\n",
    "        return cls(\n",
    "            conv_layers=[(512, 10, 5)] + [(512, 3, 2)] * 5 + [(512, 2, 2)],\n",
    "            d_model=1024,\n",
    "            nhead=16,\n",
    "            num_encoder_layers=24,\n",
    "            dim_feedforward=4096,\n",
    "            layer_drop=0.2,\n",
    "            min_temp=0.1,\n",
    "            learning_rate=3e-4,\n",
    "            num_updates=250_000,\n",
    "        )\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, conv_layers=[(512, 10, 5)] + [(512, 3, 2)] * 5 + [(512, 2, 2)]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 1  # raw audio input\n",
    "        \n",
    "        # First layer without normalization (as per paper for Librispeech)\n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels, conv_layers[0][0], conv_layers[0][1], stride=conv_layers[0][2]),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Normalize output of first layer\n",
    "        self.layer_norm = nn.LayerNorm(conv_layers[0][0])\n",
    "        \n",
    "        # Remaining layers\n",
    "        in_channels = conv_layers[0][0]\n",
    "        for out_channels, kernel_size, stride in conv_layers[1:]:\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # First layer\n",
    "        x = self.layers[0](x)\n",
    "        \n",
    "        # Normalize output of first layer (as per paper)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Remaining layers\n",
    "        for layer in self.layers[1:]:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x.transpose(1, 2)  # Return (batch_size, time_steps, channels)\n",
    "\n",
    "class ProductQuantizer(nn.Module):\n",
    "    def __init__(self, input_dim, num_groups=2, num_vars=320, temp=2.0, min_temp=0.5, temp_decay=0.999995):\n",
    "        super().__init__()\n",
    "        self.num_groups = num_groups\n",
    "        self.num_vars = num_vars\n",
    "        self.temp = temp\n",
    "        self.min_temp = min_temp\n",
    "        self.temp_decay = temp_decay\n",
    "        \n",
    "        self.vars = nn.Parameter(torch.FloatTensor(num_groups * num_vars, input_dim // num_groups))\n",
    "        nn.init.uniform_(self.vars)\n",
    "        \n",
    "        self.weight_proj = nn.Linear(input_dim, num_groups * num_vars)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        bsz, tsz, fsz = x.shape\n",
    "        \n",
    "        # Project to G x V logits\n",
    "        x = self.weight_proj(x)\n",
    "        x = x.view(bsz * tsz, self.num_groups, self.num_vars)\n",
    "        \n",
    "        if self.training:\n",
    "            # Gumbel noise\n",
    "            uniform_noise = torch.rand_like(x)\n",
    "            gumbel = -torch.log(-torch.log(uniform_noise + 1e-10) + 1e-10)\n",
    "            \n",
    "            # Apply formula: exp((l_{g,v} + n_v)/τ) / sum_k(exp((l_{g,k} + n_k)/τ))\n",
    "            logits_with_noise = (x + gumbel) / self.temp\n",
    "            numerator = torch.exp(logits_with_noise)\n",
    "            denominator = numerator.sum(dim=-1, keepdim=True)\n",
    "            x = numerator / denominator\n",
    "            \n",
    "            # Update temperature\n",
    "            self.temp = max(self.temp * self.temp_decay, self.min_temp)\n",
    "        else:\n",
    "            # During inference, use straight-through estimator\n",
    "            logits = x / self.temp\n",
    "            x = F.softmax(logits, dim=-1)\n",
    "            \n",
    "        # Straight-through Gumbel-Softmax\n",
    "        indices = x.max(dim=-1)[1]\n",
    "        x_hard = torch.zeros_like(x).scatter_(-1, indices.unsqueeze(-1), 1.0)\n",
    "        x = (x_hard - x).detach() + x\n",
    "        \n",
    "        return x.view(bsz, tsz, -1)\n",
    "\n",
    "class Wav2Vec2(nn.Module):\n",
    "    def __init__(self, config: Wav2Vec2Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        # Feature encoder with layer norm and GELU\n",
    "        self.feature_encoder = FeatureEncoder(config.conv_layers)\n",
    "        \n",
    "        # Calculate the encoder output dimension based on the last conv layer\n",
    "        last_conv_channels = config.conv_layers[-1][0]\n",
    "        \n",
    "        # Add projection layer to match transformer dimensions\n",
    "        self.proj = nn.Linear(last_conv_channels, config.d_model)\n",
    "        \n",
    "        # Add projection for quantized vectors\n",
    "        self.quantizer_proj = nn.Linear(config.num_groups * config.num_vars, config.d_model)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
    "        \n",
    "        # Context network components\n",
    "        # 1. Convolutional layer for relative positional embedding\n",
    "        kernel_size = 128\n",
    "        # Calculate padding to maintain sequence length\n",
    "        padding = kernel_size  # Full padding on both sides\n",
    "        self.context_pos_conv = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                config.d_model,\n",
    "                config.d_model,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                groups=16,\n",
    "                padding_mode='replicate'  # Use replicate padding to avoid edge effects\n",
    "            ),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # 2. Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            dim_feedforward=config.dim_feedforward,\n",
    "            dropout=config.dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, config.num_encoder_layers)\n",
    "        \n",
    "        # Quantizer\n",
    "        self.quantizer = ProductQuantizer(\n",
    "            config.d_model,\n",
    "            num_groups=config.num_groups,\n",
    "            num_vars=config.num_vars,\n",
    "            temp=config.temp,\n",
    "            min_temp=config.min_temp,\n",
    "            temp_decay=config.temp_decay\n",
    "        )\n",
    "        \n",
    "        self.mask_emb = nn.Parameter(torch.FloatTensor(config.d_model).uniform_())\n",
    "        \n",
    "    def apply_mask(self, x, mask_prob=0.065, mask_length=10):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Calculate how many starting indices to sample\n",
    "        num_mask = int(T * mask_prob)\n",
    "        \n",
    "        # Sample starting indices\n",
    "        mask_starts = torch.randperm(T)[:num_mask]\n",
    "        \n",
    "        # Create mask tensor\n",
    "        mask = torch.zeros(B, T, dtype=torch.bool, device=x.device)\n",
    "        \n",
    "        # For each starting index, mask the subsequent M time steps\n",
    "        for start in mask_starts:\n",
    "            end = min(start + mask_length, T)\n",
    "            mask[:, start:end] = True\n",
    "            \n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x, mask=True):\n",
    "        # Debug: Print input shape\n",
    "        # print(f\"\\nShape tracking:\")\n",
    "        # print(f\"Raw input: {x.shape}\")\n",
    "        \n",
    "        # 1. Feature encoder (provides initial relative positional information)\n",
    "        x = self.feature_encoder(x)\n",
    "        #print(f\"After feature encoder: {x.shape}\")\n",
    "        \n",
    "        # 2. Project to transformer dimension\n",
    "        x = self.proj(x)\n",
    "        #print(f\"After projection: {x.shape}\")\n",
    "\n",
    "        q = self.quantizer(x)\n",
    "        q = self.quantizer_proj(q)\n",
    "        \n",
    "        # 3. Initialize mask_indices\n",
    "        mask_indices = None\n",
    "        \n",
    "        # 4. Apply masking if requested\n",
    "        if mask:\n",
    "            mask_indices = self.apply_mask(\n",
    "                x,\n",
    "                mask_prob=self.config.mask_prob,\n",
    "                mask_length=self.config.mask_length\n",
    "            )\n",
    "            x = torch.where(\n",
    "                mask_indices.unsqueeze(-1),\n",
    "                self.mask_emb.view(1, 1, -1).expand(x.shape[0], -1, -1),\n",
    "                x\n",
    "            )\n",
    "        #print(f\"After masking: {x.shape}\")\n",
    "        \n",
    "        # 5. Layer normalization\n",
    "        x = self.layer_norm(x)\n",
    "        #print(f\"After layer norm: {x.shape}\")\n",
    "        \n",
    "        # 6. Context network processing\n",
    "        # Add relative positional information through convolution\n",
    "        x_t = x.transpose(1, 2)  # [B, T, C] -> [B, C, T]\n",
    "        #print(f\"Before context conv: {x_t.shape}\")\n",
    "        \n",
    "        # Save original sequence length\n",
    "        orig_len = x_t.size(2)\n",
    "        \n",
    "        # Apply convolution and ensure output length matches input\n",
    "        pos_embedding = self.context_pos_conv(x_t)  # Apply conv and GELU\n",
    "        \n",
    "        # Ensure we get exactly the sequence length we want\n",
    "        if pos_embedding.size(2) > orig_len:\n",
    "            # If too long, trim from both ends equally\n",
    "            excess = pos_embedding.size(2) - orig_len\n",
    "            start = excess // 2\n",
    "            pos_embedding = pos_embedding[:, :, start:start + orig_len]\n",
    "        elif pos_embedding.size(2) < orig_len:\n",
    "            # If too short, pad both ends equally\n",
    "            pad_size = orig_len - pos_embedding.size(2)\n",
    "            pad_left = pad_size // 2\n",
    "            pad_right = pad_size - pad_left\n",
    "            pos_embedding = F.pad(pos_embedding, (pad_left, pad_right), mode='replicate')\n",
    "            \n",
    "        #print(f\"After context conv: {pos_embedding.shape}\")\n",
    "        \n",
    "        pos_embedding = pos_embedding.transpose(1, 2)  # [B, C, T] -> [B, T, C]\n",
    "        #print(f\"After transpose: {pos_embedding.shape}\")\n",
    "        \n",
    "        # Verify shapes match before adding\n",
    "        assert x.shape == pos_embedding.shape, f\"Shape mismatch: x={x.shape}, pos_embedding={pos_embedding.shape}\"\n",
    "        x = x + pos_embedding\n",
    "        \n",
    "        # 7. Transformer processing\n",
    "        c = self.transformer(x)\n",
    "        #print(f\"After transformer: {c.shape}\")\n",
    "        \n",
    "        #print(f\"Final quantized: {q.shape}\")\n",
    "        \n",
    "        if not mask:\n",
    "            mask_indices = torch.zeros(x.shape[0], x.shape[1], dtype=torch.bool, device=x.device)\n",
    "        \n",
    "        return c, q, mask_indices\n",
    "        \n",
    "    def compute_loss(self, c, q, mask_indices, num_negatives=100, temperature=0.1, eps=1e-7):\n",
    "        \"\"\"\n",
    "        Compute contrastive loss:\n",
    "        L_m = -log(exp(sim(c_t, q_t)/κ) / sum_k(exp(sim(c_t, q_k)/κ)))\n",
    "        where:\n",
    "        - c_t is the context network output at masked position t\n",
    "        - q_t is the correct quantized representation at position t\n",
    "        - q_k are the distractors (including q_t)\n",
    "        - κ is the temperature (set to 0.1)\n",
    "        - sim(a,b) is the cosine similarity between a and b\n",
    "        \"\"\"\n",
    "        # Check if we have any masked indices\n",
    "        if mask_indices.sum() == 0:\n",
    "            return torch.tensor(0.0, device=c.device, requires_grad=True)\n",
    "        \n",
    "        # Get masked indices in flattened form\n",
    "        flat_mask = mask_indices.view(-1)\n",
    "        masked_indices = torch.nonzero(flat_mask).squeeze(-1)\n",
    "        \n",
    "        if len(masked_indices) == 0:  # No masked positions\n",
    "            return torch.tensor(0.0, device=c.device, requires_grad=True)\n",
    "        \n",
    "        # Get positive samples (c_t and q_t pairs)\n",
    "        c_masked = c.view(-1, c.size(-1))[masked_indices]  # c_t\n",
    "        q_masked = q.view(-1, q.size(-1))[masked_indices]  # q_t\n",
    "        \n",
    "        # Sample negative indices for each positive\n",
    "        with torch.no_grad():\n",
    "            neg_indices = self._sample_negatives(masked_indices, len(flat_mask), num_negatives)\n",
    "            negatives = q.view(-1, q.size(-1))[neg_indices]  # q_k distractors\n",
    "        \n",
    "        # Compute cosine similarity with numerical stability\n",
    "        c_masked = F.normalize(c_masked + eps, dim=-1)\n",
    "        q_masked = F.normalize(q_masked + eps, dim=-1)\n",
    "        negatives = F.normalize(negatives + eps, dim=-1)\n",
    "        \n",
    "        # Compute sim(c_t, q_t) for positives\n",
    "        pos_logits = torch.sum(c_masked * q_masked, dim=-1, keepdim=True)  # [num_masked, 1]\n",
    "        \n",
    "        # Compute sim(c_t, q_k) for negatives\n",
    "        neg_logits = torch.bmm(c_masked.unsqueeze(1), negatives.transpose(1, 2)).squeeze(1)  # [num_masked, num_negatives]\n",
    "        \n",
    "        # Concatenate positive and negative logits\n",
    "        logits = torch.cat([pos_logits, neg_logits], dim=1)  # [num_masked, 1 + num_negatives]\n",
    "        \n",
    "        # Scale by temperature κ\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Targets are zeros (positive pair should be selected)\n",
    "        targets = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n",
    "        \n",
    "        # Compute contrastive loss using cross entropy (equivalent to -log(exp(pos)/sum(exp(all))))\n",
    "        contrastive_loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        # Compute diversity loss (weight α = 0.1 as per paper)\n",
    "        try:\n",
    "            prob_perplexity = self.compute_prob_perplexity()\n",
    "            diversity_loss = -torch.log(prob_perplexity + eps) * 0.1  # α = 0.1\n",
    "            diversity_loss = torch.clamp(diversity_loss, min=-10, max=10)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error computing diversity loss: {e}\")\n",
    "            diversity_loss = torch.tensor(0.0, device=c.device, requires_grad=True)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = contrastive_loss + diversity_loss\n",
    "        \n",
    "        # Print loss components only if they're valid\n",
    "        # if self.training and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "        #     print(f\"\\nLoss components:\")\n",
    "        #     print(f\"Contrastive loss: {contrastive_loss.item():.4f}\")\n",
    "        #     print(f\"Diversity loss: {diversity_loss.item():.4f}\")\n",
    "        #     print(f\"Total loss: {loss.item():.4f}\")\n",
    "        #     print(f\"Prob perplexity: {prob_perplexity.item():.2f}\")\n",
    "        #     print(f\"Number of masked positions: {len(masked_indices)}\")\n",
    "        #     print(f\"Average positive logit: {pos_logits.mean().item():.4f}\")\n",
    "        #     print(f\"Average negative logit: {neg_logits.mean().item():.4f}\")\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def compute_prob_perplexity(self, eps=1e-7):\n",
    "        \"\"\"\n",
    "        Compute the perplexity of the averaged softmax probability over codebook entries\n",
    "        This helps ensure even usage of the codebook vectors\n",
    "        \"\"\"\n",
    "        # Get the weight matrix from the quantizer projection\n",
    "        logits = self.quantizer.weight_proj.weight\n",
    "        \n",
    "        # Reshape to (num_groups, num_vars, -1)\n",
    "        logits = logits.view(\n",
    "            self.config.num_groups,\n",
    "            self.config.num_vars,\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        # Compute softmax probabilities with numerical stability\n",
    "        logits = torch.clamp(logits, min=-100, max=100)\n",
    "        probs = F.softmax(logits, dim=1)  # Along codebook dimension\n",
    "        \n",
    "        # Average over feature dimension\n",
    "        avg_probs = probs.mean(dim=-1)\n",
    "        \n",
    "        # Add small epsilon to avoid log(0)\n",
    "        avg_probs = avg_probs + eps\n",
    "        \n",
    "        # Compute perplexity for each group\n",
    "        perplexities = []\n",
    "        for g in range(self.config.num_groups):\n",
    "            p = avg_probs[g]\n",
    "            # Normalize probabilities to sum to 1\n",
    "            p = p / p.sum()\n",
    "            perplexity = torch.exp(-torch.sum(p * torch.log(p)))\n",
    "            perplexities.append(perplexity)\n",
    "        \n",
    "        # Average perplexity across groups\n",
    "        avg_perplexity = torch.stack(perplexities).mean()\n",
    "        \n",
    "        return avg_perplexity\n",
    "        \n",
    "    def _sample_negatives(self, pos_indices, num_masked, num_negatives):\n",
    "        \"\"\"Sample negative indices from other masked positions.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create a range of all masked indices\n",
    "            all_indices = torch.arange(num_masked, device=pos_indices.device)\n",
    "            \n",
    "            # For each positive, sample K distractors from other masked positions\n",
    "            neg_indices = []\n",
    "            for i in range(len(pos_indices)):\n",
    "                # Exclude the current positive index\n",
    "                valid_indices = torch.cat([all_indices[:i], all_indices[i+1:]])\n",
    "                # Sample K indices\n",
    "                sampled = valid_indices[torch.randperm(len(valid_indices))[:num_negatives]]\n",
    "                neg_indices.append(sampled)\n",
    "            \n",
    "            return torch.stack(neg_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    def __init__(self, split=\"test-clean\", target_length=480000, device='cpu'):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "            root=os.path.expanduser(\"~/.cache\"),\n",
    "            url=split,\n",
    "            download=True,\n",
    "        )\n",
    "        self.device = device\n",
    "        self.target_length = target_length\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, item):\n",
    "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
    "        assert sample_rate == 16000\n",
    "        audio = audio.flatten().numpy()\n",
    "        audio_length = len(audio)\n",
    "        if audio_length < self.target_length:\n",
    "            padding = np.zeros(self.target_length - audio_length)\n",
    "            audio = np.concatenate((audio, padding))\n",
    "        elif audio_length > self.target_length:\n",
    "            audio = audio[:self.target_length]\n",
    "        audio = torch.tensor(audio, dtype=torch.float32)\n",
    "        return audio, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class WarmupLinearSchedule(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch\n",
    "        if step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            return [base_lr * step / self.warmup_steps for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # Linear decay\n",
    "            return [base_lr * (self.total_steps - step) / (self.total_steps - self.warmup_steps)\n",
    "                   for base_lr in self.base_lrs]\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Wav2Vec2,\n",
    "        train_dataset: LibriSpeech,\n",
    "        val_dataset: LibriSpeech,\n",
    "        config: Wav2Vec2Config,\n",
    "        device: torch.device,\n",
    "        is_librispeech: bool = True,\n",
    "        patience: int = 10,\n",
    "        log_dir: str = \"runs\",\n",
    "        batch_size: int = 8,\n",
    "        checkpoint_dir: str = \"checkpoints\",\n",
    "        loader_kwargs: dict = None\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.is_librispeech = is_librispeech\n",
    "        self.patience = patience\n",
    "        self.log_dir = log_dir\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Create log directory\n",
    "        self.run_dir = os.path.join(log_dir, datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize metrics tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        # Early stopping variables\n",
    "        self.patience_counter = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        \n",
    "        # Librispeech specific regularization\n",
    "        if is_librispeech:\n",
    "            self.encoder_grad_scale = 0.1\n",
    "            self.l2_regularization = True\n",
    "        else:\n",
    "            self.encoder_grad_scale = 1.0\n",
    "            self.l2_regularization = False\n",
    "            \n",
    "        # Use provided loader kwargs or default\n",
    "        if loader_kwargs is None:\n",
    "            loader_kwargs = {\n",
    "                'batch_size': batch_size,\n",
    "                'num_workers': 2 if device.type == 'cuda' else 0,\n",
    "                'pin_memory': device.type == 'cuda',\n",
    "            }\n",
    "            \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            **loader_kwargs\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # Setup learning rate scheduler\n",
    "        total_steps = 400000 if config.d_model == 768 else 250000  # BASE vs LARGE\n",
    "        warmup_steps = int(0.08 * total_steps)  # 8% warmup\n",
    "        self.scheduler = WarmupLinearSchedule(\n",
    "            self.optimizer,\n",
    "            warmup_steps=warmup_steps,\n",
    "            total_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def save_checkpoint(self, is_best=False):\n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates,\n",
    "            'config': self.config,\n",
    "        }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        latest_path = os.path.join(self.checkpoint_dir, \"latest_checkpoint.pt\")\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save best model if needed\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.checkpoint_dir, \"best_model.pt\")\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"Saved best model with validation loss: {self.best_val_loss:.4f}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']\n",
    "        self.train_losses = checkpoint['train_losses']\n",
    "        self.val_losses = checkpoint['val_losses']\n",
    "        self.learning_rates = checkpoint['learning_rates']\n",
    "        print(f\"Loaded checkpoint from epoch {self.current_epoch}\")\n",
    "    \n",
    "    def save_metrics(self):\n",
    "        metrics = {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates\n",
    "        }\n",
    "        with open(os.path.join(self.run_dir, 'metrics.json'), 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "            \n",
    "    def plot_metrics(self):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.title('Training and Validation Losses')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot learning rate\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(self.learning_rates, label='Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.run_dir, 'training_metrics.png'))\n",
    "        plt.close()\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc=\"Training\")):\n",
    "            audio = batch[0].to(self.device)\n",
    "            \n",
    "            # Print shapes for debugging (only first batch)\n",
    "            if batch_idx == 0:\n",
    "                print(f\"\\nInput audio shape: {audio.shape}\")\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                c, q, mask_indices = self.model(audio)\n",
    "                \n",
    "                # Print shapes for debugging (only first batch)\n",
    "                if batch_idx == 0:\n",
    "                    print(f\"Context output shape: {c.shape}\")\n",
    "                    print(f\"Quantized output shape: {q.shape}\")\n",
    "                    print(f\"Mask indices shape: {mask_indices.shape}\")\n",
    "                    print(f\"Number of masked positions: {mask_indices.sum().item()}\")\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.model.compute_loss(c, q, mask_indices)\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    print(f\"Initial loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # Add L2 regularization for Librispeech\n",
    "                if self.l2_regularization:\n",
    "                    l2_loss = 0.0\n",
    "                    for name, param in self.model.feature_encoder.named_parameters():\n",
    "                        if 'weight' in name:\n",
    "                            l2_loss += torch.norm(param)\n",
    "                    loss += 0.01 * l2_loss\n",
    "                    \n",
    "                    if batch_idx == 0:\n",
    "                        print(f\"L2 loss: {l2_loss.item():.4f}\")\n",
    "                        print(f\"Total loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Scale gradients for feature encoder if using Librispeech\n",
    "                if self.is_librispeech:\n",
    "                    for param in self.model.feature_encoder.parameters():\n",
    "                        param.grad *= self.encoder_grad_scale\n",
    "                \n",
    "                # Clip gradients\n",
    "                grad_norm = clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "                if batch_idx == 0:\n",
    "                    print(f\"Gradient norm: {grad_norm:.4f}\")\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                # Track learning rate\n",
    "                self.learning_rates.append(self.scheduler.get_last_lr()[0])\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Print batch statistics (only first few batches)\n",
    "                if batch_idx < 5:\n",
    "                    print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"\\nError in batch {batch_idx}:\")\n",
    "                print(f\"Input shape: {audio.shape}\")\n",
    "                raise e\n",
    "            \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(tqdm(self.val_loader, desc=\"Validation\")):\n",
    "                audio = batch[0].to(self.device)\n",
    "                \n",
    "                try:\n",
    "                    # Forward pass with masking enabled (same as training)\n",
    "                    c, q, mask_indices = self.model(audio, mask=True)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = self.model.compute_loss(c, q, mask_indices)\n",
    "                    \n",
    "                    # Add L2 regularization if using Librispeech (same as training)\n",
    "                    if self.l2_regularization:\n",
    "                        l2_loss = 0.0\n",
    "                        for name, param in self.model.feature_encoder.named_parameters():\n",
    "                            if 'weight' in name:\n",
    "                                l2_loss += torch.norm(param)\n",
    "                        loss += 0.01 * l2_loss\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    # Print validation statistics (only first batch)\n",
    "                    if batch_idx == 0:\n",
    "                        print(f\"\\nValidation batch statistics:\")\n",
    "                        print(f\"Loss: {loss.item():.4f}\")\n",
    "                        print(f\"Number of masked positions: {mask_indices.sum().item()}\")\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"\\nError in validation batch {batch_idx}:\")\n",
    "                    print(f\"Input shape: {audio.shape}\")\n",
    "                    raise e\n",
    "                \n",
    "        return total_loss / len(self.val_loader)\n",
    "        \n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(self.current_epoch, num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss = self.train_epoch()\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self.validate()\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch + 1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Check for improvement\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(is_best)\n",
    "            \n",
    "            # Save and plot metrics\n",
    "            self.save_metrics()\n",
    "            self.plot_metrics()\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "            \n",
    "            self.current_epoch = epoch + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec 2.0 Training\n",
    "\n",
    "This notebook provides an interface to train the Wav2Vec 2.0 model on LibriSpeech dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Device and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA RTX A6000\n",
      "Memory Available: 50.93 GB\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Set batch size based on device\n",
    "batch_size = 32 if torch.cuda.is_available() else 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 28539\n",
      "Validation dataset size: 2703\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation datasets\n",
    "train_dataset = LibriSpeech(split=\"train-clean-100\", target_length=48000)\n",
    "val_dataset = LibriSpeech(split=\"dev-clean\", target_length=48000)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 95.87M\n"
     ]
    }
   ],
   "source": [
    "# Configure dataset size and model size for fast testing\n",
    "FAST_DEV = False  # Set to False for full training\n",
    "\n",
    "if FAST_DEV:\n",
    "    # Use smaller model\n",
    "    config = Wav2Vec2Config.TINY()\n",
    "\n",
    "    # Use smaller subset of data\n",
    "    train_subset_size = 1000  # Adjust this number as needed\n",
    "    val_subset_size = 100\n",
    "\n",
    "    # Create subset indices\n",
    "    train_indices = torch.randperm(len(train_dataset))[:train_subset_size]\n",
    "    val_indices = torch.randperm(len(val_dataset))[:val_subset_size]\n",
    "\n",
    "    # Create subset datasets\n",
    "    from torch.utils.data import Subset\n",
    "    train_dataset = Subset(train_dataset, train_indices)\n",
    "    val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "    # Use smaller batch size\n",
    "    batch_size = 4\n",
    "\n",
    "    print(f\"Fast dev mode enabled:\")\n",
    "    print(f\"Training on {len(train_dataset)} examples\")\n",
    "    print(f\"Validating on {len(val_dataset)} examples\")\n",
    "else:\n",
    "    # Use original BASE configuration\n",
    "    config = Wav2Vec2Config.BASE()\n",
    "\n",
    "# Reduce model size if using CPU\n",
    "if device.type == 'cpu':\n",
    "    config.d_model = 256\n",
    "    config.dim_feedforward = 1024\n",
    "    config.num_encoder_layers = 4\n",
    "\n",
    "# Create model and move to device\n",
    "model = Wav2Vec2(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Enable multi-GPU if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    config=config,\n",
    "    device=device,\n",
    "    is_librispeech=True,\n",
    "    patience=10,\n",
    "    log_dir=\"wav2vec_runs\",\n",
    "    batch_size=batch_size,\n",
    "    checkpoint_dir=checkpoint_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Checkpoint (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoints/latest_checkpoint.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2463948/2293057000.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 6\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint if it exists\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pt\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    trainer.load_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Training\n",
    "\n",
    "The training will automatically:\n",
    "- Save checkpoints\n",
    "- Plot training metrics\n",
    "- Implement early stopping\n",
    "- Handle GPU memory efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input audio shape: torch.Size([32, 48000])\n",
      "Raw input: torch.Size([32, 48000])\n",
      "Context output shape: torch.Size([32, 149, 768])\n",
      "Quantized output shape: torch.Size([32, 149, 768])\n",
      "Mask indices shape: torch.Size([32, 149])\n",
      "Number of masked positions: 2240\n",
      "Initial loss: 4.0382\n",
      "L2 loss: 11.0398\n",
      "Total loss: 4.1486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/892 [00:05<1:24:40,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 0.3450\n",
      "Batch 0, Loss: 4.1486\n",
      "Raw input: torch.Size([32, 48000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/892 [00:06<43:09,  2.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: 4.1488\n",
      "Raw input: torch.Size([32, 48000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/892 [00:07<28:08,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2, Loss: 4.1485\n",
      "Raw input: torch.Size([32, 48000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4/892 [00:08<23:16,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3, Loss: 4.1489\n",
      "Raw input: torch.Size([32, 48000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/892 [00:09<19:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4, Loss: 4.1485\n",
      "Raw input: torch.Size([32, 48000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 6/892 [00:10<24:38,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input: torch.Size([32, 48000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Will stop early if no improvement\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 292\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 189\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInput audio shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     c, q, mask_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Print shapes for debugging (only first batch)\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 337\u001b[0m, in \u001b[0;36mWav2Vec2.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    334\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m pos_embedding\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# 7. Transformer processing\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m#print(f\"After transformer: {c.shape}\")\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m#print(f\"Final quantized: {q.shape}\")\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:904\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    900\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    903\u001b[0m         x\n\u001b[0;32m--> 904\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     )\n\u001b[1;32m    906\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:918\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    913\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    917\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:6097\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   6094\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   6095\u001b[0m         in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6096\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 6097\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6098\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6099\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   6100\u001b[0m         q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6101\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train(num_epochs=25)  # Will stop early if no improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final training metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(trainer.train_losses, label='Train Loss')\n",
    "plt.plot(trainer.val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(trainer.learning_rates, label='Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
