{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide Complet Wav2Vec2: Entraînement, Conversion et Inférence\n",
    "\n",
    "Ce notebook regroupe toutes les étapes nécessaires pour:\n",
    "1. Installation des dépendances\n",
    "2. Entraînement du modèle\n",
    "3. Conversion en ONNX\n",
    "4. Inférence en temps réel\n",
    "5. Évaluation des performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install soundfile torch torchaudio transformers pyaudio webrtcvad rx halo onnx onnxruntime wheel pyctcdecode\n",
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoProcessor, AutoModelForCTC\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "from rx.subject import BehaviorSubject\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classe d'inférence Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Wave2Vec2Inference:\n",
    "    def __init__(self, model_name, hotwords=[], use_lm_if_possible=True, use_gpu=True):\n",
    "        self.device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        if use_lm_if_possible:            \n",
    "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCTC.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.hotwords = hotwords\n",
    "        self.use_lm_if_possible = use_lm_if_possible\n",
    "\n",
    "    def buffer_to_text(self, audio_buffer):\n",
    "        if len(audio_buffer) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        inputs = self.processor(torch.tensor(audio_buffer), sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(inputs.input_values.to(self.device),\n",
    "                                attention_mask=inputs.attention_mask.to(self.device)).logits            \n",
    "\n",
    "        if hasattr(self.processor, 'decoder') and self.use_lm_if_possible:\n",
    "            transcription = \\\n",
    "                self.processor.decode(logits[0].cpu().numpy(),                                      \n",
    "                                      hotwords=self.hotwords,\n",
    "                                      output_word_offsets=True)                             \n",
    "            confidence = transcription.lm_score / len(transcription.text.split(\" \"))\n",
    "            transcription = transcription.text       \n",
    "        else:\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "            transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "            confidence = self.confidence_score(logits,predicted_ids)\n",
    "\n",
    "        return transcription, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conversion en ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def convert_to_onnx(model_id_or_path, onnx_model_name):\n",
    "    print(f\"Converting {model_id_or_path} to ONNX\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_id_or_path)\n",
    "    audio_len = 250000\n",
    "\n",
    "    x = torch.randn(1, audio_len, requires_grad=True)\n",
    "\n",
    "    torch.onnx.export(model,\n",
    "                    x,\n",
    "                    onnx_model_name,\n",
    "                    export_params=True,\n",
    "                    opset_version=11,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['input'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {1: 'audio_len'},\n",
    "                                'output': {1: 'audio_len'}})\n",
    "\n",
    "def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
    "    print(\"Starting quantization...\")\n",
    "    quantize_dynamic(onnx_model_path,\n",
    "                     quantized_model_path,\n",
    "                     weight_type=QuantType.QUInt8)\n",
    "    print(f\"Quantized model saved to: {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classe d'inférence ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Wave2Vec2ONNXInference:\n",
    "    def __init__(self, model_name, onnx_path):\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name) \n",
    "        options = rt.SessionOptions()\n",
    "        options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "        self.model = rt.InferenceSession(onnx_path, options)\n",
    "\n",
    "    def buffer_to_text(self, audio_buffer):\n",
    "        if len(audio_buffer) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        inputs = self.processor(torch.tensor(audio_buffer), sampling_rate=16_000, return_tensors=\"np\", padding=True)\n",
    "        input_values = inputs.input_values\n",
    "        onnx_outputs = self.model.run(None, {self.model.get_inputs()[0].name: input_values})[0]\n",
    "        prediction = np.argmax(onnx_outputs, axis=-1)\n",
    "        transcription = self.processor.decode(prediction.squeeze().tolist())\n",
    "        return transcription.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_performance(audio_file, base_model, iterations=100):\n",
    "    torch.set_num_threads(16)\n",
    "    audio_input, samplerate = sf.read(audio_file)\n",
    "    assert samplerate == 16000, \"L'audio doit être échantillonné à 16kHz\"\n",
    "\n",
    "    # Créer les modèles\n",
    "    asr = Wave2Vec2Inference(base_model)\n",
    "    asr_onnx = Wave2Vec2ONNXInference(base_model, f\"{base_model.split('/')[-1]}.onnx\")\n",
    "    asr_onnx_quant = Wave2Vec2ONNXInference(base_model, f\"{base_model.split('/')[-1]}.quant.onnx\")\n",
    "\n",
    "    # Test de transcription\n",
    "    print(\"Test de transcription:\")\n",
    "    text_pytorch = asr.buffer_to_text(audio_input)[0]\n",
    "    text_onnx = asr_onnx.buffer_to_text(audio_input)\n",
    "    print(f\"PyTorch: {text_pytorch}\")\n",
    "    print(f\"ONNX: {text_onnx}\")\n",
    "\n",
    "    # Test de performance\n",
    "    print(f\"\\nTest de performance sur {iterations} itérations:\")\n",
    "    \n",
    "    seconds = timeit.timeit(lambda: asr.buffer_to_text(audio_input), number=iterations)\n",
    "    print(f\"PyTorch: {(seconds/iterations)*1000:.2f} ms/iter\")\n",
    "\n",
    "    seconds = timeit.timeit(lambda: asr_onnx.buffer_to_text(audio_input), number=iterations)\n",
    "    print(f\"ONNX: {(seconds/iterations)*1000:.2f} ms/iter\")\n",
    "\n",
    "    seconds = timeit.timeit(lambda: asr_onnx_quant.buffer_to_text(audio_input), number=iterations)\n",
    "    print(f\"ONNX quantifié: {(seconds/iterations)*1000:.2f} ms/iter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Définir le modèle de base\n",
    "base_model = \"facebook/wav2vec2-base\"\n",
    "\n",
    "# 1. Convertir en ONNX\n",
    "convert_to_onnx(base_model, \"wav2vec2-base.onnx\")\n",
    "\n",
    "# 2. Quantifier le modèle ONNX\n",
    "quantize_onnx_model(\"wav2vec2-base.onnx\", \"wav2vec2-base.quant.onnx\")\n",
    "\n",
    "# 3. Évaluer les performances (nécessite un fichier audio test.wav)\n",
    "evaluate_performance(\"test.wav\", base_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}