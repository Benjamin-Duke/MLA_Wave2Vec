{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Wav2Vec2Config:\n",
        "    # Feature encoder\n",
        "    conv_layers: list = None\n",
        "    dropout: float = 0.1\n",
        "    layer_drop: float = 0.05\n",
        "\n",
        "    # Transformer\n",
        "    d_model: int = 768\n",
        "    nhead: int = 8\n",
        "    num_encoder_layers: int = 12\n",
        "    dim_feedforward: int = 3072\n",
        "\n",
        "    # Quantizer\n",
        "    num_groups: int = 2\n",
        "    num_vars: int = 320\n",
        "    temp: float = 2.0\n",
        "    min_temp: float = 0.5\n",
        "    temp_decay: float = 0.999995\n",
        "\n",
        "    # Masking\n",
        "    mask_prob: float = 0.065\n",
        "    mask_length: int = 10\n",
        "\n",
        "    # Training\n",
        "    learning_rate: float = 5e-4\n",
        "\n",
        "    @classmethod\n",
        "    def BASE(cls):\n",
        "        return cls(\n",
        "            conv_layers=[(512, 10, 5)] + [(512, 3, 2)] * 5 + [(512, 2, 2)],\n",
        "            layer_drop=0.05,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def TINY(cls):\n",
        "        \"\"\"Tiny configuration for fast testing\"\"\"\n",
        "        return cls(\n",
        "            conv_layers=[(256, 10, 5)] + [(256, 3, 2)] * 3 + [(256, 2, 2)],  # Fewer layers, smaller channels\n",
        "            d_model=256,  # Smaller model dimension\n",
        "            nhead=4,      # Fewer attention heads\n",
        "            num_encoder_layers=4,  # Fewer transformer layers\n",
        "            dim_feedforward=1024,  # Smaller feedforward dimension\n",
        "            layer_drop=0.05,\n",
        "            num_groups=2,\n",
        "            num_vars=160,  # Smaller codebook\n",
        "            learning_rate=1e-3,  # Slightly higher learning rate for faster convergence\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def LARGE(cls):\n",
        "        return cls(\n",
        "            conv_layers=[(512, 10, 5)] + [(512, 3, 2)] * 5 + [(512, 2, 2)],\n",
        "            d_model=1024,\n",
        "            nhead=16,\n",
        "            num_encoder_layers=24,\n",
        "            dim_feedforward=4096,\n",
        "            layer_drop=0.2,\n",
        "            min_temp=0.1,\n",
        "            learning_rate=3e-4,\n",
        "        )\n",
        "\n",
        "class FeatureEncoder(nn.Module):\n",
        "    def __init__(self, conv_layers=[(512, 10, 5)] + [(512, 3, 2)] * 5 + [(512, 2, 2)]):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_channels = 1  # raw audio input\n",
        "        for out_channels, kernel_size, stride in conv_layers:\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride),\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.GELU(),\n",
        "                    nn.Dropout(0.1),\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x.transpose(1, 2)  # Return (batch_size, time_steps, channels)\n",
        "\n",
        "class ProductQuantizer(nn.Module):\n",
        "    def __init__(self, input_dim, num_groups=2, num_vars=320, temp=2.0, min_temp=0.5, temp_decay=0.999995):\n",
        "        super().__init__()\n",
        "        self.num_groups = num_groups\n",
        "        self.num_vars = num_vars\n",
        "        self.temp = temp\n",
        "        self.min_temp = min_temp\n",
        "        self.temp_decay = temp_decay\n",
        "\n",
        "        self.vars = nn.Parameter(torch.FloatTensor(num_groups * num_vars, input_dim // num_groups))\n",
        "        nn.init.uniform_(self.vars)\n",
        "\n",
        "        self.weight_proj = nn.Linear(input_dim, num_groups * num_vars)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        bsz, tsz, fsz = x.shape\n",
        "\n",
        "        # Project to G x V logits\n",
        "        x = self.weight_proj(x)\n",
        "        x = x.view(bsz * tsz, self.num_groups, self.num_vars)\n",
        "\n",
        "        if self.training:\n",
        "            # Gumbel noise\n",
        "            uniform_noise = torch.rand_like(x)\n",
        "            gumbel = -torch.log(-torch.log(uniform_noise + 1e-10) + 1e-10)\n",
        "\n",
        "            # Apply formula: exp((l_{g,v} + n_v)/τ) / sum_k(exp((l_{g,k} + n_k)/τ))\n",
        "            logits_with_noise = (x + gumbel) / self.temp\n",
        "            numerator = torch.exp(logits_with_noise)\n",
        "            denominator = numerator.sum(dim=-1, keepdim=True)\n",
        "            x = numerator / denominator\n",
        "\n",
        "            # Update temperature\n",
        "            self.temp = max(self.temp * self.temp_decay, self.min_temp)\n",
        "        else:\n",
        "            # During inference, use straight-through estimator\n",
        "            logits = x / self.temp\n",
        "            x = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Straight-through Gumbel-Softmax\n",
        "        indices = x.max(dim=-1)[1]\n",
        "        x_hard = torch.zeros_like(x).scatter_(-1, indices.unsqueeze(-1), 1.0)\n",
        "        x = (x_hard - x).detach() + x\n",
        "\n",
        "        return x.view(bsz, tsz, -1)\n",
        "\n",
        "class Wav2Vec2(nn.Module):\n",
        "    def __init__(self, config: Wav2Vec2Config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        # Feature encoder with layer norm and GELU\n",
        "        self.feature_encoder = FeatureEncoder(config.conv_layers)\n",
        "\n",
        "        # Calculate the encoder output dimension based on the last conv layer\n",
        "        last_conv_channels = config.conv_layers[-1][0]\n",
        "\n",
        "        # Add projection layer to match transformer dimensions\n",
        "        self.proj = nn.Linear(last_conv_channels, config.d_model)\n",
        "\n",
        "        # Add projection for quantized vectors\n",
        "        self.quantizer_proj = nn.Linear(config.num_groups * config.num_vars, config.d_model)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        # Context network components\n",
        "        # 1. Convolutional layer for relative positional embedding\n",
        "        kernel_size = 128\n",
        "        # Calculate padding to maintain sequence length\n",
        "        padding = kernel_size  # Full padding on both sides\n",
        "        self.context_pos_conv = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                config.d_model,\n",
        "                config.d_model,\n",
        "                kernel_size=kernel_size,\n",
        "                padding=padding,\n",
        "                groups=16,\n",
        "                padding_mode='replicate'  # Use replicate padding to avoid edge effects\n",
        "            ),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # 2. Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config.d_model,\n",
        "            nhead=config.nhead,\n",
        "            dim_feedforward=config.dim_feedforward,\n",
        "            dropout=config.dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, config.num_encoder_layers)\n",
        "\n",
        "        # Quantizer\n",
        "        self.quantizer = ProductQuantizer(\n",
        "            config.d_model,\n",
        "            num_groups=config.num_groups,\n",
        "            num_vars=config.num_vars,\n",
        "            temp=config.temp,\n",
        "            min_temp=config.min_temp,\n",
        "            temp_decay=config.temp_decay\n",
        "        )\n",
        "\n",
        "        self.mask_emb = nn.Parameter(torch.FloatTensor(config.d_model).uniform_())\n",
        "\n",
        "    def apply_mask(self, x, mask_prob=0.065, mask_length=10):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Calculate how many starting indices to sample\n",
        "        num_mask = int(T * mask_prob)\n",
        "\n",
        "        # Sample starting indices\n",
        "        mask_starts = torch.randperm(T)[:num_mask]\n",
        "\n",
        "        # Create mask tensor\n",
        "        mask = torch.zeros(B, T, dtype=torch.bool, device=x.device)\n",
        "\n",
        "        # For each starting index, mask the subsequent M time steps\n",
        "        for start in mask_starts:\n",
        "            end = min(start + mask_length, T)\n",
        "            mask[:, start:end] = True\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, mask=True):\n",
        "        # Debug: Print input shape\n",
        "        # print(f\"\\nShape tracking:\")\n",
        "        # print(f\"Raw input: {x.shape}\")\n",
        "\n",
        "        # 1. Feature encoder (provides initial relative positional information)\n",
        "        x = self.feature_encoder(x)\n",
        "        # print(f\"After feature encoder: {x.shape}\")\n",
        "\n",
        "        # 2. Project to transformer dimension\n",
        "        x = self.proj(x)\n",
        "        # print(f\"After projection: {x.shape}\")\n",
        "\n",
        "        # 3. Initialize mask_indices\n",
        "        mask_indices = None\n",
        "\n",
        "        # 4. Apply masking if requested\n",
        "        if mask:\n",
        "            mask_indices = self.apply_mask(\n",
        "                x,\n",
        "                mask_prob=self.config.mask_prob,\n",
        "                mask_length=self.config.mask_length\n",
        "            )\n",
        "            x = torch.where(\n",
        "                mask_indices.unsqueeze(-1),\n",
        "                self.mask_emb.view(1, 1, -1).expand(x.shape[0], -1, -1),\n",
        "                x\n",
        "            )\n",
        "        # print(f\"After masking: {x.shape}\")\n",
        "\n",
        "        # 5. Layer normalization\n",
        "        x = self.layer_norm(x)\n",
        "        # print(f\"After layer norm: {x.shape}\")\n",
        "\n",
        "        # 6. Context network processing\n",
        "        # Add relative positional information through convolution\n",
        "        x_t = x.transpose(1, 2)  # [B, T, C] -> [B, C, T]\n",
        "        # print(f\"Before context conv: {x_t.shape}\")\n",
        "\n",
        "        # Save original sequence length\n",
        "        orig_len = x_t.size(2)\n",
        "\n",
        "        # Apply convolution and ensure output length matches input\n",
        "        pos_embedding = self.context_pos_conv(x_t)  # Apply conv and GELU\n",
        "\n",
        "        # Ensure we get exactly the sequence length we want\n",
        "        if pos_embedding.size(2) > orig_len:\n",
        "            # If too long, trim from both ends equally\n",
        "            excess = pos_embedding.size(2) - orig_len\n",
        "            start = excess // 2\n",
        "            pos_embedding = pos_embedding[:, :, start:start + orig_len]\n",
        "        elif pos_embedding.size(2) < orig_len:\n",
        "            # If too short, pad both ends equally\n",
        "            pad_size = orig_len - pos_embedding.size(2)\n",
        "            pad_left = pad_size // 2\n",
        "            pad_right = pad_size - pad_left\n",
        "            pos_embedding = F.pad(pos_embedding, (pad_left, pad_right), mode='replicate')\n",
        "\n",
        "        # print(f\"After context conv: {pos_embedding.shape}\")\n",
        "\n",
        "        pos_embedding = pos_embedding.transpose(1, 2)  # [B, C, T] -> [B, T, C]\n",
        "        # print(f\"After transpose: {pos_embedding.shape}\")\n",
        "\n",
        "        # Verify shapes match before adding\n",
        "        assert x.shape == pos_embedding.shape, f\"Shape mismatch: x={x.shape}, pos_embedding={pos_embedding.shape}\"\n",
        "        x = x + pos_embedding\n",
        "\n",
        "        # 7. Transformer processing\n",
        "        c = self.transformer(x)\n",
        "        # print(f\"After transformer: {c.shape}\")\n",
        "\n",
        "        # 8. Quantization\n",
        "        q = self.quantizer(c)\n",
        "        q = self.quantizer_proj(q)\n",
        "        # print(f\"Final quantized: {q.shape}\")\n",
        "\n",
        "        if not mask:\n",
        "            mask_indices = torch.zeros(x.shape[0], x.shape[1], dtype=torch.bool, device=x.device)\n",
        "\n",
        "        return c, q, mask_indices\n",
        "\n",
        "    def compute_loss(self, c, q, mask_indices, eps=1e-7):\n",
        "        \"\"\"\n",
        "        Compute contrastive loss and diversity loss for wav2vec 2.0 training\n",
        "        \"\"\"\n",
        "        # Check if we have any masked indices\n",
        "        if mask_indices.sum() == 0:\n",
        "            return torch.tensor(0.0, device=c.device, requires_grad=True)\n",
        "\n",
        "        # Only compute loss on masked indices\n",
        "        c = c[mask_indices]\n",
        "        q = q[mask_indices]\n",
        "\n",
        "        if c.size(0) == 0:  # No masked positions\n",
        "            return torch.tensor(0.0, device=c.device, requires_grad=True)\n",
        "\n",
        "        # Compute cosine similarity with numerical stability\n",
        "        c = F.normalize(c + eps, dim=-1)\n",
        "        q = F.normalize(q + eps, dim=-1)\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        temperature = 0.1\n",
        "        logits = torch.matmul(c, q.transpose(0, 1)) / temperature\n",
        "\n",
        "        # Clamp logits for numerical stability\n",
        "        logits = torch.clamp(logits, min=-100, max=100)\n",
        "        targets = torch.arange(logits.size(0), device=logits.device)\n",
        "\n",
        "        contrastive_loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # Compute diversity loss with safety checks\n",
        "        try:\n",
        "            prob_perplexity = self.compute_prob_perplexity()\n",
        "            diversity_loss = -torch.log(prob_perplexity + eps) * 0.1\n",
        "\n",
        "            # Clamp diversity loss for stability\n",
        "            diversity_loss = torch.clamp(diversity_loss, min=-10, max=10)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error computing diversity loss: {e}\")\n",
        "            diversity_loss = torch.tensor(0.0, device=c.device, requires_grad=True)\n",
        "\n",
        "        # Total loss\n",
        "        loss = contrastive_loss + diversity_loss\n",
        "\n",
        "        # Print loss components only if they're valid\n",
        "        # if self.training and not torch.isnan(loss) and not torch.isinf(loss):\n",
        "        #     print(f\"\\nLoss components:\")\n",
        "        #     print(f\"Contrastive loss: {contrastive_loss.item():.4f}\")\n",
        "        #     print(f\"Diversity loss: {diversity_loss.item():.4f}\")\n",
        "        #     print(f\"Total loss: {loss.item():.4f}\")\n",
        "        #     print(f\"Prob perplexity: {prob_perplexity.item():.2f}\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def compute_prob_perplexity(self, eps=1e-7):\n",
        "        \"\"\"\n",
        "        Compute the perplexity of the averaged softmax probability over codebook entries\n",
        "        This helps ensure even usage of the codebook vectors\n",
        "        \"\"\"\n",
        "        # Get the weight matrix from the quantizer projection\n",
        "        logits = self.quantizer.weight_proj.weight\n",
        "\n",
        "        # Reshape to (num_groups, num_vars, -1)\n",
        "        logits = logits.view(\n",
        "            self.config.num_groups,\n",
        "            self.config.num_vars,\n",
        "            -1\n",
        "        )\n",
        "\n",
        "        # Compute softmax probabilities with numerical stability\n",
        "        logits = torch.clamp(logits, min=-100, max=100)\n",
        "        probs = F.softmax(logits, dim=1)  # Along codebook dimension\n",
        "\n",
        "        # Average over feature dimension\n",
        "        avg_probs = probs.mean(dim=-1)\n",
        "\n",
        "        # Add small epsilon to avoid log(0)\n",
        "        avg_probs = avg_probs + eps\n",
        "\n",
        "        # Compute perplexity for each group\n",
        "        perplexities = []\n",
        "        for g in range(self.config.num_groups):\n",
        "            p = avg_probs[g]\n",
        "            # Normalize probabilities to sum to 1\n",
        "            p = p / p.sum()\n",
        "            perplexity = torch.exp(-torch.sum(p * torch.log(p)))\n",
        "            perplexities.append(perplexity)\n",
        "\n",
        "        # Average perplexity across groups\n",
        "        avg_perplexity = torch.stack(perplexities).mean()\n",
        "\n",
        "        return avg_perplexity\n",
        "\n",
        "    def _sample_negatives(self, pos_indices, num_masked, num_negatives):\n",
        "        \"\"\"Sample negative indices from other masked positions.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Create a range of all masked indices\n",
        "            all_indices = torch.arange(num_masked, device=pos_indices.device)\n",
        "\n",
        "            # For each positive, sample K distractors from other masked positions\n",
        "            neg_indices = []\n",
        "            for i in range(len(pos_indices)):\n",
        "                # Exclude the current positive index\n",
        "                valid_indices = torch.cat([all_indices[:i], all_indices[i+1:]])\n",
        "                # Sample K indices\n",
        "                sampled = valid_indices[torch.randperm(len(valid_indices))[:num_negatives]]\n",
        "                neg_indices.append(sampled)\n",
        "\n",
        "            return torch.stack(neg_indices)"
      ],
      "metadata": {
        "id": "czcysG3BVymc"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "class LibriSpeech(torch.utils.data.Dataset):\n",
        "    def __init__(self, split=\"test-clean\", target_length=480000, device='cpu'):\n",
        "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
        "            root=os.path.expanduser(\"~/.cache\"),\n",
        "            url=split,\n",
        "            download=True,\n",
        "        )\n",
        "        self.device = device\n",
        "        self.target_length = target_length\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    def __getitem__(self, item):\n",
        "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
        "        assert sample_rate == 16000\n",
        "        audio = audio.flatten().numpy()\n",
        "        audio_length = len(audio)\n",
        "        if audio_length < self.target_length:\n",
        "            padding = np.zeros(self.target_length - audio_length)\n",
        "            audio = np.concatenate((audio, padding))\n",
        "        elif audio_length > self.target_length:\n",
        "            audio = audio[:self.target_length]\n",
        "        audio = torch.tensor(audio, dtype=torch.float32)\n",
        "        return audio, text"
      ],
      "metadata": {
        "id": "IRZY8Y83Wfkm"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "class WarmupLinearSchedule(optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps\n",
        "        super(WarmupLinearSchedule, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        step = self.last_epoch\n",
        "        if step < self.warmup_steps:\n",
        "            # Linear warmup\n",
        "            return [base_lr * step / self.warmup_steps for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            # Linear decay\n",
        "            return [base_lr * (self.total_steps - step) / (self.total_steps - self.warmup_steps)\n",
        "                   for base_lr in self.base_lrs]\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Wav2Vec2,\n",
        "        train_dataset: LibriSpeech,\n",
        "        val_dataset: LibriSpeech,\n",
        "        config: Wav2Vec2Config,\n",
        "        device: torch.device,\n",
        "        is_librispeech: bool = True,\n",
        "        patience: int = 10,\n",
        "        log_dir: str = \"runs\",\n",
        "        batch_size: int = 8,\n",
        "        checkpoint_dir: str = \"checkpoints\",\n",
        "        loader_kwargs: dict = None\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.config = config\n",
        "        self.is_librispeech = is_librispeech\n",
        "        self.patience = patience\n",
        "        self.log_dir = log_dir\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        # Create log directory\n",
        "        self.run_dir = os.path.join(log_dir, datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "        os.makedirs(self.run_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize metrics tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.learning_rates = []\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        # Early stopping variables\n",
        "        self.patience_counter = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "        # Librispeech specific regularization\n",
        "        if is_librispeech:\n",
        "            self.encoder_grad_scale = 0.1\n",
        "            self.l2_regularization = True\n",
        "        else:\n",
        "            self.encoder_grad_scale = 1.0\n",
        "            self.l2_regularization = False\n",
        "\n",
        "        # Use provided loader kwargs or default\n",
        "        if loader_kwargs is None:\n",
        "            loader_kwargs = {\n",
        "                'batch_size': batch_size,\n",
        "                'num_workers': 2 if device.type == 'cuda' else 0,\n",
        "                'pin_memory': device.type == 'cuda',\n",
        "            }\n",
        "\n",
        "        # Create data loaders\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            shuffle=True,\n",
        "            **loader_kwargs\n",
        "        )\n",
        "\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            shuffle=False,\n",
        "            **loader_kwargs\n",
        "        )\n",
        "\n",
        "        # Setup optimizer\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        # Setup learning rate scheduler\n",
        "        total_steps = 400000 if config.d_model == 768 else 250000  # BASE vs LARGE\n",
        "        warmup_steps = int(0.08 * total_steps)  # 8% warmup\n",
        "        self.scheduler = WarmupLinearSchedule(\n",
        "            self.optimizer,\n",
        "            warmup_steps=warmup_steps,\n",
        "            total_steps=total_steps\n",
        "        )\n",
        "\n",
        "    def save_checkpoint(self, is_best=False):\n",
        "        checkpoint = {\n",
        "            'epoch': self.current_epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'learning_rates': self.learning_rates,\n",
        "            'config': self.config,\n",
        "        }\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        latest_path = os.path.join(self.checkpoint_dir, \"latest_checkpoint.pt\")\n",
        "        torch.save(checkpoint, latest_path)\n",
        "\n",
        "        # Save best model if needed\n",
        "        if is_best:\n",
        "            best_path = os.path.join(self.checkpoint_dir, \"best_model.pt\")\n",
        "            torch.save(checkpoint, best_path)\n",
        "            print(f\"Saved best model with validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.current_epoch = checkpoint['epoch']\n",
        "        self.best_val_loss = checkpoint['best_val_loss']\n",
        "        self.train_losses = checkpoint['train_losses']\n",
        "        self.val_losses = checkpoint['val_losses']\n",
        "        self.learning_rates = checkpoint['learning_rates']\n",
        "        print(f\"Loaded checkpoint from epoch {self.current_epoch}\")\n",
        "\n",
        "    def save_metrics(self):\n",
        "        metrics = {\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'learning_rates': self.learning_rates\n",
        "        }\n",
        "        with open(os.path.join(self.run_dir, 'metrics.json'), 'w') as f:\n",
        "            json.dump(metrics, f)\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot losses\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "        plt.plot(self.val_losses, label='Validation Loss')\n",
        "        plt.title('Training and Validation Losses')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot learning rate\n",
        "        plt.subplot(2, 1, 2)\n",
        "        plt.plot(self.learning_rates, label='Learning Rate')\n",
        "        plt.title('Learning Rate Schedule')\n",
        "        plt.xlabel('Step')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.run_dir, 'training_metrics.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc=\"Training\")):\n",
        "            audio = batch[0].to(self.device)\n",
        "\n",
        "            # Print shapes for debugging (only first batch)\n",
        "            if batch_idx == 0:\n",
        "                print(f\"\\nInput audio shape: {audio.shape}\")\n",
        "\n",
        "            try:\n",
        "                # Forward pass\n",
        "                c, q, mask_indices = self.model(audio)\n",
        "\n",
        "                # Print shapes for debugging (only first batch)\n",
        "                if batch_idx == 0:\n",
        "                    print(f\"Context output shape: {c.shape}\")\n",
        "                    print(f\"Quantized output shape: {q.shape}\")\n",
        "                    print(f\"Mask indices shape: {mask_indices.shape}\")\n",
        "                    print(f\"Number of masked positions: {mask_indices.sum().item()}\")\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.model.compute_loss(c, q, mask_indices)\n",
        "\n",
        "                if batch_idx == 0:\n",
        "                    print(f\"Initial loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Add L2 regularization for Librispeech\n",
        "                if self.l2_regularization:\n",
        "                    l2_loss = 0.0\n",
        "                    for name, param in self.model.feature_encoder.named_parameters():\n",
        "                        if 'weight' in name:\n",
        "                            l2_loss += torch.norm(param)\n",
        "                    loss += 0.01 * l2_loss\n",
        "\n",
        "                    if batch_idx == 0:\n",
        "                        print(f\"L2 loss: {l2_loss.item():.4f}\")\n",
        "                        print(f\"Total loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Backward pass\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                # Scale gradients for feature encoder if using Librispeech\n",
        "                if self.is_librispeech:\n",
        "                    for param in self.model.feature_encoder.parameters():\n",
        "                        param.grad *= self.encoder_grad_scale\n",
        "\n",
        "                # Clip gradients\n",
        "                grad_norm = clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
        "                if batch_idx == 0:\n",
        "                    print(f\"Gradient norm: {grad_norm:.4f}\")\n",
        "\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "\n",
        "                # Track learning rate\n",
        "                self.learning_rates.append(self.scheduler.get_last_lr()[0])\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Print batch statistics (only first few batches)\n",
        "                if batch_idx < 5:\n",
        "                    print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"\\nError in batch {batch_idx}:\")\n",
        "                print(f\"Input shape: {audio.shape}\")\n",
        "                raise e\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(tqdm(self.val_loader, desc=\"Validation\")):\n",
        "                audio = batch[0].to(self.device)\n",
        "\n",
        "                # Forward pass with masking enabled\n",
        "                c, q, mask_indices = self.model(audio, mask=True)\n",
        "\n",
        "                # Print shapes for debugging (only first batch)\n",
        "                if batch_idx == 0:\n",
        "                    print(f\"\\nValidation batch shapes:\")\n",
        "                    print(f\"Context output shape: {c.shape}\")\n",
        "                    print(f\"Quantized output shape: {q.shape}\")\n",
        "                    print(f\"Mask indices shape: {mask_indices.shape}\")\n",
        "                    print(f\"Number of masked positions: {mask_indices.sum().item()}\")\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.model.compute_loss(c, q, mask_indices)\n",
        "\n",
        "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
        "                    total_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "\n",
        "                    if batch_idx == 0:\n",
        "                        print(f\"First validation batch loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Compute average loss\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        print(f\"\\nValidation stats:\")\n",
        "        print(f\"Total batches: {num_batches}\")\n",
        "        print(f\"Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        for epoch in range(self.current_epoch, num_epochs):\n",
        "            self.current_epoch = epoch\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            # Training\n",
        "            train_loss = self.train_epoch()\n",
        "            self.train_losses.append(train_loss)\n",
        "            print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "            # Validation\n",
        "            val_loss = self.validate()\n",
        "            self.val_losses.append(val_loss)\n",
        "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.patience_counter = 0\n",
        "                self.save_checkpoint(is_best=True)\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                self.save_checkpoint(is_best=False)\n",
        "\n",
        "            # Save current metrics and plot\n",
        "            self.save_metrics()\n",
        "            self.plot_metrics()\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.patience:\n",
        "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
        "                break\n",
        "\n",
        "def main():\n",
        "    # Set device and print capabilities\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Adjust batch size based on device\n",
        "    batch_size = 32 if torch.cuda.is_available() else 2  # Larger batch size for GPU\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = LibriSpeech(split=\"train-clean-100\", target_length=48000)\n",
        "    val_dataset = LibriSpeech(split=\"dev-clean\", target_length=48000)\n",
        "\n",
        "    # Create model (BASE or LARGE)\n",
        "    config = Wav2Vec2Config.BASE()  # Use BASE for CPU\n",
        "    if device.type == 'cpu':\n",
        "        # Reduce model size for CPU training\n",
        "        config.d_model = 256\n",
        "        config.dim_feedforward = 1024\n",
        "        config.num_encoder_layers = 4\n",
        "\n",
        "    # Create model and move to device\n",
        "    model = Wav2Vec2(config)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Enable multi-GPU if available\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = \"checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        config=config,\n",
        "        device=device,\n",
        "        is_librispeech=True,\n",
        "        patience=10,\n",
        "        log_dir=\"wav2vec_runs\",\n",
        "        batch_size=batch_size,\n",
        "        checkpoint_dir=checkpoint_dir\n",
        "    )\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pt\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        trainer.load_checkpoint(checkpoint_path)\n",
        "\n",
        "    # Start training\n",
        "    trainer.train(num_epochs=100)  # Will stop early if no improvement"
      ],
      "metadata": {
        "id": "s6nwPZICWC1l"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjS1Mqe0VreD"
      },
      "source": [
        "# Wav2Vec 2.0 Training\n",
        "\n",
        "This notebook provides an interface to train the Wav2Vec 2.0 model on LibriSpeech dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "3ADIgW7cVreF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1KQkQetVreG"
      },
      "source": [
        "## 1. Setup Device and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMycwmnCVreH",
        "outputId": "7a7a9945-5190-4324-8bb4-02cebdd2d131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory Available: 15.84 GB\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Set batch size based on device\n",
        "batch_size = 32 if torch.cuda.is_available() else 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoitpY8GVreH"
      },
      "source": [
        "## 2. Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqxlhohaVreI",
        "outputId": "51c1a7cd-1232-456d-ce57-91c9487e453a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 28539\n",
            "Validation dataset size: 2703\n"
          ]
        }
      ],
      "source": [
        "# Create training and validation datasets\n",
        "train_dataset = LibriSpeech(split=\"train-clean-100\", target_length=48000)\n",
        "val_dataset = LibriSpeech(split=\"dev-clean\", target_length=48000)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNHg7QPMVreJ"
      },
      "source": [
        "## 3. Create and Configure Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHyYQiYCVreK",
        "outputId": "d75c5058-8c96-4294-9446-118cc8fddedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast dev mode enabled:\n",
            "Training on 1000 examples\n",
            "Validating on 100 examples\n",
            "Model parameters: 4.68M\n"
          ]
        }
      ],
      "source": [
        "# Configure dataset size and model size for fast testing\n",
        "FAST_DEV = True  # Set to False for full training\n",
        "\n",
        "if FAST_DEV:\n",
        "    # Use smaller model\n",
        "    config = Wav2Vec2Config.TINY()\n",
        "\n",
        "    # Use smaller subset of data\n",
        "    train_subset_size = 1000  # Adjust this number as needed\n",
        "    val_subset_size = 100\n",
        "\n",
        "    # Create subset indices\n",
        "    train_indices = torch.randperm(len(train_dataset))[:train_subset_size]\n",
        "    val_indices = torch.randperm(len(val_dataset))[:val_subset_size]\n",
        "\n",
        "    # Create subset datasets\n",
        "    from torch.utils.data import Subset\n",
        "    train_dataset = Subset(train_dataset, train_indices)\n",
        "    val_dataset = Subset(val_dataset, val_indices)\n",
        "\n",
        "    # Use smaller batch size\n",
        "    batch_size = 4\n",
        "\n",
        "    print(f\"Fast dev mode enabled:\")\n",
        "    print(f\"Training on {len(train_dataset)} examples\")\n",
        "    print(f\"Validating on {len(val_dataset)} examples\")\n",
        "else:\n",
        "    # Use original BASE configuration\n",
        "    config = Wav2Vec2Config.BASE()\n",
        "\n",
        "# Reduce model size if using CPU\n",
        "if device.type == 'cpu':\n",
        "    config.d_model = 256\n",
        "    config.dim_feedforward = 1024\n",
        "    config.num_encoder_layers = 4\n",
        "\n",
        "# Create model and move to device\n",
        "model = Wav2Vec2(config)\n",
        "model = model.to(device)\n",
        "\n",
        "# Enable multi-GPU if available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LFDO66VreK"
      },
      "source": [
        "## 4. Setup Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "Cb0HxBxzVreK"
      },
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "checkpoint_dir = \"checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    config=config,\n",
        "    device=device,\n",
        "    is_librispeech=True,\n",
        "    patience=10,\n",
        "    log_dir=\"wav2vec_runs\",\n",
        "    batch_size=batch_size,\n",
        "    checkpoint_dir=checkpoint_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XChQ0iq-VreL"
      },
      "source": [
        "## 5. Load Checkpoint (if exists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "q3eSMltFVreL"
      },
      "outputs": [],
      "source": [
        "# Load checkpoint if it exists\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pt\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    trainer.load_checkpoint(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjw7MBlCVreL"
      },
      "source": [
        "## 6. Start Training\n",
        "\n",
        "The training will automatically:\n",
        "- Save checkpoints\n",
        "- Plot training metrics\n",
        "- Implement early stopping\n",
        "- Handle GPU memory efficiently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJjifqoPVreL",
        "outputId": "260f9993-5c32-45e9-b052-bea9c987f6f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/250 [00:00<01:41,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1140\n",
            "Initial loss: 6.6755\n",
            "L2 loss: 126.1106\n",
            "Total loss: 7.9366\n",
            "Gradient norm: 0.9030\n",
            "Batch 0, Loss: 7.9366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   1%|          | 3/250 [00:00<00:42,  5.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1, Loss: 7.9270\n",
            "Batch 2, Loss: 7.9642\n",
            "Batch 3, Loss: 7.9118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|▎         | 7/250 [00:00<00:24, 10.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 4, Loss: 7.9619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:18<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 7.8511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1252\n",
            "First validation batch loss: 6.6247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 6.5403\n",
            "Validation Loss: 6.5403\n",
            "Saved best model with validation loss: 6.5403\n",
            "\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1168\n",
            "Initial loss: 6.5670\n",
            "L2 loss: 125.3745\n",
            "Total loss: 7.8207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:57,  4.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 0.2676\n",
            "Batch 0, Loss: 7.8207\n",
            "Batch 1, Loss: 7.7393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 7.6795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 7.7676\n",
            "Batch 4, Loss: 7.7803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 13.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 7.7803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1196\n",
            "First validation batch loss: 6.5734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 6.5308\n",
            "Validation Loss: 6.5308\n",
            "Saved best model with validation loss: 6.5308\n",
            "\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1172\n",
            "Initial loss: 6.5680\n",
            "L2 loss: 122.8120\n",
            "Total loss: 7.7961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:54,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 0.1889\n",
            "Batch 0, Loss: 7.7961\n",
            "Batch 1, Loss: 7.7945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  8.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 7.7778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 7.8126\n",
            "Batch 4, Loss: 7.7416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 7.7563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:04,  5.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1084\n",
            "First validation batch loss: 6.4809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 30.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 6.5260\n",
            "Validation Loss: 6.5260\n",
            "Saved best model with validation loss: 6.5260\n",
            "\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1188\n",
            "Initial loss: 6.5744\n",
            "L2 loss: 118.3121\n",
            "Total loss: 7.7576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:58,  4.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 0.1659\n",
            "Batch 0, Loss: 7.7576\n",
            "Batch 1, Loss: 7.6963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 7.7222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 7.7839\n",
            "Batch 4, Loss: 7.7531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 7.7010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1196\n",
            "First validation batch loss: 6.5792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 6.5653\n",
            "Validation Loss: 6.5653\n",
            "\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1144\n",
            "Initial loss: 6.5370\n",
            "L2 loss: 112.1801\n",
            "Total loss: 7.6588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:56,  4.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 0.1613\n",
            "Batch 0, Loss: 7.6588\n",
            "Batch 1, Loss: 7.6395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 7.6728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 7.6613\n",
            "Batch 4, Loss: 7.5849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:20<00:00, 12.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 7.2301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1044\n",
            "First validation batch loss: 5.0575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 5.0736\n",
            "Validation Loss: 5.0736\n",
            "Saved best model with validation loss: 5.0736\n",
            "\n",
            "Epoch 6/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1200\n",
            "Initial loss: 5.2097\n",
            "L2 loss: 105.3851\n",
            "Total loss: 6.2636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:52,  4.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 1.9662\n",
            "Batch 0, Loss: 6.2636\n",
            "Batch 1, Loss: 6.2365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  9.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 6.1999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 6.2528\n",
            "Batch 4, Loss: 6.3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 5.6635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1076\n",
            "First validation batch loss: 3.9846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 33.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 4.0137\n",
            "Validation Loss: 4.0137\n",
            "Saved best model with validation loss: 4.0137\n",
            "\n",
            "Epoch 7/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1132\n",
            "Initial loss: 4.1715\n",
            "L2 loss: 98.8801\n",
            "Total loss: 5.1603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:53,  4.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 1.8082\n",
            "Batch 0, Loss: 5.1603\n",
            "Batch 1, Loss: 5.1577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 5.2311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 5.0945\n",
            "Batch 4, Loss: 5.1412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.9315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1176\n",
            "First validation batch loss: 3.5837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 3.5953\n",
            "Validation Loss: 3.5953\n",
            "Saved best model with validation loss: 3.5953\n",
            "\n",
            "Epoch 8/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1152\n",
            "Initial loss: 3.7118\n",
            "L2 loss: 92.7078\n",
            "Total loss: 4.6389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:58,  4.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 2.1160\n",
            "Batch 0, Loss: 4.6389\n",
            "Batch 1, Loss: 4.5764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 4.5778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 4.5701\n",
            "Batch 4, Loss: 4.7143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.5138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1056\n",
            "First validation batch loss: 3.2416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 3.3561\n",
            "Validation Loss: 3.3561\n",
            "Saved best model with validation loss: 3.3561\n",
            "\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<01:03,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 3.4823\n",
            "L2 loss: 86.8762\n",
            "Total loss: 4.3511\n",
            "Gradient norm: 2.5340\n",
            "Batch 0, Loss: 4.3511\n",
            "Batch 1, Loss: 4.3384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 5/250 [00:00<00:26,  9.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 4.3638\n",
            "Batch 3, Loss: 4.4294\n",
            "Batch 4, Loss: 4.4523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 4.1799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1212\n",
            "First validation batch loss: 3.1559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 33.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 3.0708\n",
            "Validation Loss: 3.0708\n",
            "Saved best model with validation loss: 3.0708\n",
            "\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1044\n",
            "Initial loss: 3.0955\n",
            "L2 loss: 81.1404\n",
            "Total loss: 3.9069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:56,  4.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 3.5690\n",
            "Batch 0, Loss: 3.9069\n",
            "Batch 1, Loss: 4.0498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 4.0336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 4.0329\n",
            "Batch 4, Loss: 3.9166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.8669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1168\n",
            "First validation batch loss: 2.8785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.9061\n",
            "Validation Loss: 2.9061\n",
            "Saved best model with validation loss: 2.9061\n",
            "\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/250 [00:00<01:19,  3.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1192\n",
            "Initial loss: 2.9852\n",
            "L2 loss: 75.5457\n",
            "Total loss: 3.7407\n",
            "Gradient norm: 3.5834\n",
            "Batch 0, Loss: 3.7407\n",
            "Batch 1, Loss: 3.7288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 5/250 [00:00<00:27,  8.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 3.7938\n",
            "Batch 3, Loss: 3.7389\n",
            "Batch 4, Loss: 3.7977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.6076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1104\n",
            "First validation batch loss: 2.7427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.7187\n",
            "Validation Loss: 2.7187\n",
            "Saved best model with validation loss: 2.7187\n",
            "\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1208\n",
            "Initial loss: 2.8164\n",
            "L2 loss: 71.4620\n",
            "Total loss: 3.5310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:55,  4.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 3.6729\n",
            "Batch 0, Loss: 3.5310\n",
            "Batch 1, Loss: 3.5112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 3.3906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 3.4170\n",
            "Batch 4, Loss: 3.5777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.3945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1012\n",
            "First validation batch loss: 2.5583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.5710\n",
            "Validation Loss: 2.5710\n",
            "Saved best model with validation loss: 2.5710\n",
            "\n",
            "Epoch 13/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<01:06,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 2.7071\n",
            "L2 loss: 68.8841\n",
            "Total loss: 3.3960\n",
            "Gradient norm: 3.2970\n",
            "Batch 0, Loss: 3.3960\n",
            "Batch 1, Loss: 3.3564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:32,  7.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 3.2777\n",
            "Batch 3, Loss: 3.3294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:25,  9.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 4, Loss: 3.3910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.2604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1256\n",
            "First validation batch loss: 2.5013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 30.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.4882\n",
            "Validation Loss: 2.4882\n",
            "Saved best model with validation loss: 2.4882\n",
            "\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1180\n",
            "Initial loss: 2.5074\n",
            "L2 loss: 64.5136\n",
            "Total loss: 3.1525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:54,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 3.2606\n",
            "Batch 0, Loss: 3.1525\n",
            "Batch 1, Loss: 3.1290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  8.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 3.0375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 3.1267\n",
            "Batch 4, Loss: 3.1399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.1428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1208\n",
            "First validation batch loss: 2.4408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.5347\n",
            "Validation Loss: 2.5347\n",
            "\n",
            "Epoch 15/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<01:16,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1196\n",
            "Initial loss: 2.4479\n",
            "L2 loss: 62.7277\n",
            "Total loss: 3.0752\n",
            "Gradient norm: 10.9789\n",
            "Batch 0, Loss: 3.0752\n",
            "Batch 1, Loss: 3.1170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:34,  7.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 3.0985\n",
            "Batch 3, Loss: 2.9775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:26,  9.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 4, Loss: 3.1172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.0635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1316\n",
            "First validation batch loss: 2.4922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.4410\n",
            "Validation Loss: 2.4410\n",
            "Saved best model with validation loss: 2.4410\n",
            "\n",
            "Epoch 16/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1128\n",
            "Initial loss: 2.3934\n",
            "L2 loss: 60.5262\n",
            "Total loss: 2.9987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:56,  4.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 4.0215\n",
            "Batch 0, Loss: 2.9987\n",
            "Batch 1, Loss: 3.0588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 3.1203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.8985\n",
            "Batch 4, Loss: 3.0277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.9323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1156\n",
            "First validation batch loss: 2.3418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 33.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.4410\n",
            "Validation Loss: 2.4410\n",
            "Saved best model with validation loss: 2.4410\n",
            "\n",
            "Epoch 17/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1168\n",
            "Initial loss: 2.3499\n",
            "L2 loss: 57.0485\n",
            "Total loss: 2.9204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:59,  4.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 7.8503\n",
            "Batch 0, Loss: 2.9204\n",
            "Batch 1, Loss: 3.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:30,  8.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.9526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:24,  9.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.9419\n",
            "Batch 4, Loss: 2.8005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.8243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1128\n",
            "First validation batch loss: 2.2912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.3863\n",
            "Validation Loss: 2.3863\n",
            "Saved best model with validation loss: 2.3863\n",
            "\n",
            "Epoch 18/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1092\n",
            "Initial loss: 2.1992\n",
            "L2 loss: 53.1702\n",
            "Total loss: 2.7309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:58,  4.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 10.7721\n",
            "Batch 0, Loss: 2.7309\n",
            "Batch 1, Loss: 2.9124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.8818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.8621\n",
            "Batch 4, Loss: 2.7793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.7259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1144\n",
            "First validation batch loss: 2.2383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.2471\n",
            "Validation Loss: 2.2471\n",
            "Saved best model with validation loss: 2.2471\n",
            "\n",
            "Epoch 19/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/250 [00:00<01:27,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1256\n",
            "Initial loss: 2.2204\n",
            "L2 loss: 49.3809\n",
            "Total loss: 2.7142\n",
            "Gradient norm: 7.6273\n",
            "Batch 0, Loss: 2.7142\n",
            "Batch 1, Loss: 2.6065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 5/250 [00:00<00:28,  8.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.7053\n",
            "Batch 3, Loss: 2.5731\n",
            "Batch 4, Loss: 2.6188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.6304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1168\n",
            "First validation batch loss: 2.2722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.3310\n",
            "Validation Loss: 2.3310\n",
            "\n",
            "Epoch 20/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1200\n",
            "Initial loss: 2.2562\n",
            "L2 loss: 45.7991\n",
            "Total loss: 2.7142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:53,  4.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 7.3570\n",
            "Batch 0, Loss: 2.7142\n",
            "Batch 1, Loss: 2.5758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  8.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.6655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.6120\n",
            "Batch 4, Loss: 2.6207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.6200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1084\n",
            "First validation batch loss: 2.3594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.4799\n",
            "Validation Loss: 2.4799\n",
            "\n",
            "Epoch 21/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/250 [00:00<01:27,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1208\n",
            "Initial loss: 2.1127\n",
            "L2 loss: 46.6324\n",
            "Total loss: 2.5790\n",
            "Gradient norm: 5.3777\n",
            "Batch 0, Loss: 2.5790\n",
            "Batch 1, Loss: 2.5957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 5/250 [00:00<00:29,  8.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.5226\n",
            "Batch 3, Loss: 2.5123\n",
            "Batch 4, Loss: 2.4073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.5008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1032\n",
            "First validation batch loss: 2.1106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.4369\n",
            "Validation Loss: 2.4369\n",
            "\n",
            "Epoch 22/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1196\n",
            "Initial loss: 2.1412\n",
            "L2 loss: 41.5460\n",
            "Total loss: 2.5567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:56,  4.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 10.6432\n",
            "Batch 0, Loss: 2.5567\n",
            "Batch 1, Loss: 2.4006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.4970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.4651\n",
            "Batch 4, Loss: 2.4278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.4372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:  16%|█▌        | 4/25 [00:00<00:01, 14.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1116\n",
            "First validation batch loss: 2.3082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:01<00:00, 24.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.2389\n",
            "Validation Loss: 2.2389\n",
            "Saved best model with validation loss: 2.2389\n",
            "\n",
            "Epoch 23/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 1/250 [00:00<01:20,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1108\n",
            "Initial loss: 1.9993\n",
            "L2 loss: 39.9266\n",
            "Total loss: 2.3986\n",
            "Gradient norm: 4.5988\n",
            "Batch 0, Loss: 2.3986\n",
            "Batch 1, Loss: 2.4898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 5/250 [00:00<00:26,  9.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.3811\n",
            "Batch 3, Loss: 2.3987\n",
            "Batch 4, Loss: 2.4566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1092\n",
            "First validation batch loss: 2.4209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.2217\n",
            "Validation Loss: 2.2217\n",
            "Saved best model with validation loss: 2.2217\n",
            "\n",
            "Epoch 24/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1228\n",
            "Initial loss: 2.0143\n",
            "L2 loss: 37.8438\n",
            "Total loss: 2.3927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:56,  4.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 3.2389\n",
            "Batch 0, Loss: 2.3927\n",
            "Batch 1, Loss: 2.2799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.2946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.3626\n",
            "Batch 4, Loss: 2.3345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.2886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:04,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1148\n",
            "First validation batch loss: 3.5869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:01<00:00, 24.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 3.5683\n",
            "Validation Loss: 3.5683\n",
            "\n",
            "Epoch 25/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1156\n",
            "Initial loss: 1.9373\n",
            "L2 loss: 33.5719\n",
            "Total loss: 2.2730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<01:01,  4.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 13.3892\n",
            "Batch 0, Loss: 2.2730\n",
            "Batch 1, Loss: 2.2847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:29,  8.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.2941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.2183\n",
            "Batch 4, Loss: 2.3201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.3112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1220\n",
            "First validation batch loss: 4.4837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 4.0273\n",
            "Validation Loss: 4.0273\n",
            "\n",
            "Epoch 26/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1184\n",
            "Initial loss: 1.8715\n",
            "L2 loss: 30.2605\n",
            "Total loss: 2.1741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:57,  4.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 4.9094\n",
            "Batch 0, Loss: 2.1741\n",
            "Batch 1, Loss: 2.2225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.2987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.2529\n",
            "Batch 4, Loss: 2.2004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.2289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:  16%|█▌        | 4/25 [00:00<00:01, 14.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1164\n",
            "First validation batch loss: 2.9006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 27.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 3.2459\n",
            "Validation Loss: 3.2459\n",
            "\n",
            "Epoch 27/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1096\n",
            "Initial loss: 1.8552\n",
            "L2 loss: 26.4651\n",
            "Total loss: 2.1198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:53,  4.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 13.7310\n",
            "Batch 0, Loss: 2.1198\n",
            "Batch 1, Loss: 2.1671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.0775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.1932\n",
            "Batch 4, Loss: 2.0497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.1422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1216\n",
            "First validation batch loss: 3.1341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.9088\n",
            "Validation Loss: 2.9088\n",
            "\n",
            "Epoch 28/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1128\n",
            "Initial loss: 1.8259\n",
            "L2 loss: 22.8091\n",
            "Total loss: 2.0540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:57,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 3.5998\n",
            "Batch 0, Loss: 2.0540\n",
            "Batch 1, Loss: 2.1642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.0611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 2.1554\n",
            "Batch 4, Loss: 2.3241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.0420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:  24%|██▍       | 6/25 [00:00<00:00, 19.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1072\n",
            "First validation batch loss: 3.9128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 28.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 4.1612\n",
            "Validation Loss: 4.1612\n",
            "\n",
            "Epoch 29/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1164\n",
            "Initial loss: 1.7991\n",
            "L2 loss: 20.3353\n",
            "Total loss: 2.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:55,  4.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 15.3309\n",
            "Batch 0, Loss: 2.0024\n",
            "Batch 1, Loss: 1.9701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  8.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 1.9692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 1.9632\n",
            "Batch 4, Loss: 2.0051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.0440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1172\n",
            "First validation batch loss: 2.3375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 33.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 2.5271\n",
            "Validation Loss: 2.5271\n",
            "\n",
            "Epoch 30/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1112\n",
            "Initial loss: 1.7234\n",
            "L2 loss: 18.4201\n",
            "Total loss: 1.9076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:55,  4.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 6.0321\n",
            "Batch 0, Loss: 1.9076\n",
            "Batch 1, Loss: 1.9277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  8.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.0192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 1.8576\n",
            "Batch 4, Loss: 1.9797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.9386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  7.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1220\n",
            "First validation batch loss: 3.7594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 3.0476\n",
            "Validation Loss: 3.0476\n",
            "\n",
            "Epoch 31/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1276\n",
            "Initial loss: 1.8117\n",
            "L2 loss: 16.9731\n",
            "Total loss: 1.9814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<01:00,  4.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 5.9341\n",
            "Batch 0, Loss: 1.9814\n",
            "Batch 1, Loss: 1.9316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:28,  8.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 1.9190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 1.9246\n",
            "Batch 4, Loss: 1.9973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.9087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1216\n",
            "First validation batch loss: 3.0733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 3.3014\n",
            "Validation Loss: 3.3014\n",
            "\n",
            "Epoch 32/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1120\n",
            "Initial loss: 1.7918\n",
            "L2 loss: 15.7460\n",
            "Total loss: 1.9492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:58,  4.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 43.3786\n",
            "Batch 0, Loss: 1.9492\n",
            "Batch 1, Loss: 2.0218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:29,  8.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.1286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:23, 10.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 1.9913\n",
            "Batch 4, Loss: 1.9420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.8917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1152\n",
            "First validation batch loss: 5.0771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 32.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 4.6879\n",
            "Validation Loss: 4.6879\n",
            "\n",
            "Epoch 33/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input audio shape: torch.Size([4, 48000])\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1208\n",
            "Initial loss: 1.8820\n",
            "L2 loss: 14.5898\n",
            "Total loss: 2.0279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 1/250 [00:00<00:54,  4.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 18.3385\n",
            "Batch 0, Loss: 2.0279\n",
            "Batch 1, Loss: 1.9248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   1%|          | 3/250 [00:00<00:27,  9.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2, Loss: 2.1145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   2%|▏         | 5/250 [00:00<00:22, 10.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3, Loss: 1.9916\n",
            "Batch 4, Loss: 2.1061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 250/250 [00:19<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.8827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   4%|▍         | 1/25 [00:00<00:03,  6.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation batch shapes:\n",
            "Context output shape: torch.Size([4, 599, 256])\n",
            "Quantized output shape: torch.Size([4, 599, 256])\n",
            "Mask indices shape: torch.Size([4, 599])\n",
            "Number of masked positions: 1144\n",
            "First validation batch loss: 4.1023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 25/25 [00:00<00:00, 31.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation stats:\n",
            "Total batches: 25\n",
            "Average loss: 4.4364\n",
            "Validation Loss: 4.4364\n",
            "\n",
            "Early stopping triggered after 33 epochs\n"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "trainer.train(num_epochs=50)  # Will stop early if no improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGiPA3EhVreL"
      },
      "source": [
        "## 7. Visualize Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "IR9a2A5zVreM",
        "outputId": "ec1312a6-ef34-41ad-f592-c8a3706f3625"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot final training metrics\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot losses\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(trainer.train_losses, label='Train Loss')\n",
        "plt.plot(trainer.val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot learning rate\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(trainer.learning_rates, label='Learning Rate')\n",
        "plt.title('Learning Rate Schedule')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UP0LNnBvjvA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}